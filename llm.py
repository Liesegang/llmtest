import os
from llama_cpp import Llama

class LocalLLM:
    def __init__(self, model_path: str, context_size: int = 512, gpu_layers: int = -1):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")

        print(f"ğŸ§  LLM Loading... ({model_path})")
        
        # --- é«˜é€ŸåŒ–è¨­å®š ---
        self.llm = Llama(
            model_path=model_path,
            n_gpu_layers=gpu_layers, # -1 = GPUãƒ•ãƒ«ä½¿ç”¨
            
            # ã€é«˜é€ŸåŒ–1ã€‘ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4096ã ã¨ãƒ¡ãƒ¢ãƒªã‚’é£Ÿã†ã®ã§ã€é›‘è«‡ç¨‹åº¦ãªã‚‰2048ã§ååˆ†é«˜é€Ÿã«ãªã‚Šã¾ã™
            n_ctx=context_size,      
            
            # ã€é«˜é€ŸåŒ–2ã€‘ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™
            # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚å¤§ãã„æ–¹ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ãŒé€Ÿã„ï¼ˆVRAMã¯é£Ÿã†ï¼‰
            n_batch=1024,

            # ã€é«˜é€ŸåŒ–3ã€‘Flash Attentionæœ‰åŠ¹åŒ– (çˆ†é€ŸåŒ–ã®è¦)
            # å¯¾å¿œã—ã¦ã„ã‚Œã°åŠ‡çš„ã«é€Ÿããªã‚Šã¾ã™
            flash_attn=True, 

            verbose=True  # â˜…GPUä½¿ç”¨ãƒ­ã‚°ã‚’è¦‹ã‚‹ãŸã‚ã«Trueã«ã™ã‚‹
        )
        
        # --- GPUä½¿ç”¨ç¢ºèªãƒ­ã‚¸ãƒƒã‚¯ ---
        print("âœ… LLM Ready")

    def generate_stream(self, prompt: str, system_prompt: str = None):
        if system_prompt is None:
            system_prompt = "You are the chat assistant like Amazon echo, Siri and Google assistant. Please answer my questions as conversation in English shortly."

        formatted_prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            f"<|im_start|>user\n{prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        
        stream = self.llm(
            formatted_prompt,
            max_tokens=1024,
            stop=["<|im_end|>"],
            stream=True,
            temperature=0.7
        )
        
        buffer = ""
        delimiters = ["ã€‚", "ï¼", "ï¼Ÿ", "\n", "!", "?", "."]

        for output in stream:
            token = output['choices'][0]['text']
            buffer += token
            if any(d in token for d in delimiters):
                yield buffer
                buffer = ""
        
        if buffer.strip():
            yield buffer

# --- å‹•ä½œç¢ºèªç”¨ ---
if __name__ == "__main__":
    # ãƒ‘ã‚¹ã¯ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
    MODEL_PATH = "model_assets/qwen2.5-14b-instruct-q4_k_m-00001-of-00003.gguf"
    
    bot = LocalLLM(MODEL_PATH)
    
    print("\n--- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆãƒ†ã‚¹ãƒˆ ---")
    for sentence in bot.generate_stream("Pythonã§FizzBuzzã‚’æ›¸ã„ã¦"):
        print(f"å—ä¿¡: {sentence}") # ã“ã“ã§TTSã« sentence ã‚’æŠ•ã’ã‚Œã°OK